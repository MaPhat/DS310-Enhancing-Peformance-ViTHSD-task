{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyvi.ViTokenizer import ViTokenizer\n",
    "import random\n",
    "import re\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đọc lại\n",
    "with open('word_net_vietnamese.json', 'r', encoding='utf-8') as file:\n",
    "    list_synonyms_vn = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"religion_creed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = 'vietnamese-stopwords.txt'\n",
    "\n",
    "with open(STOPWORDS, \"r\", encoding=\"utf-8\") as ins:\n",
    "    stopwords = []\n",
    "    for line in ins:\n",
    "        dd = line.strip('\\n')\n",
    "        stopwords.append(dd)\n",
    "    stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Định nghĩa hàm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"’\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stopwords]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tword = word.replace(\"_\", \" \")\n",
    "\tif(word in list_synonyms_vn.keys() and len(list_synonyms_vn[word]) > 0):\n",
    "\t\treturn list_synonyms_vn[word]\n",
    "\treturn []\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "def random_deletion(words, p):\n",
    "\n",
    "\t#obviously, if there's only one word, don't delete it\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\t#randomly delete words with probability p\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\t#if you end up deleting all words, just return a random word\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "\treturn new_words\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(sentence, alpha_sr=0.5, alpha_ri=0.5, alpha_rs=0.5, p_rd=0.5, num_aug=30):\n",
    "    # Tokenize câu tiếng Việt\n",
    "    sentence = ViTokenizer.tokenize(sentence)\n",
    "    words = sentence.split(' ')\n",
    "    words = [word for word in words if word != '']\n",
    "    num_words = len(words)\n",
    "\n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = int(num_aug / 4) + 1\n",
    "\n",
    "    # Synonym Replacement (Thay thế từ đồng nghĩa)\n",
    "    if alpha_sr > 0:\n",
    "        n_sr = max(1, int(alpha_sr * num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = synonym_replacement(words, n_sr)\n",
    "            a_words = [a_word.replace(\"_\", \" \") for a_word in a_words]\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    # Random Insertion (Chèn ngẫu nhiên)\n",
    "    if alpha_ri > 0:\n",
    "        n_ri = max(1, int(alpha_ri * num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = random_insertion(words, n_ri)\n",
    "            a_words = [a_word.replace(\"_\", \" \") for a_word in a_words]\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    # Random Swap (Hoán đổi ngẫu nhiên)\n",
    "    if alpha_rs > 0:\n",
    "        n_rs = max(1, int(alpha_rs * num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = random_swap(words, n_rs)\n",
    "            a_words = [a_word.replace(\"_\", \" \") for a_word in a_words]\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    # Random Deletion (Xóa ngẫu nhiên)\n",
    "    if p_rd > 0:\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = random_deletion(words, p_rd)\n",
    "            a_words = [a_word.replace(\"_\", \" \") for a_word in a_words]\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    # augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "    # # shuffle(augmented_sentences)\n",
    "\n",
    "\t# #trim so that we have the desired number of augmented sentences\n",
    "    # if num_aug >= 1:\n",
    "    #     augmented_sentences = augmented_sentences[:num_aug]\n",
    "    # else:\n",
    "    #     keep_prob = num_aug / len(augmented_sentences)\n",
    "    #     augmented_sentences = [s for s in augmented_sentences if random.uniform(0,1) < keep_prob]\n",
    "\n",
    "\t#append the original sentence\n",
    "    augmented_sentences.append(sentence)\n",
    "    # Trả về các câu đã tăng cường\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augumentation = pd.DataFrame(columns=[\"content\", \"individual\", \"groups\", \"religion/creed\", \"race/ethnicity\", \"politics\"])\n",
    "for idx in range(len(df)):\n",
    "    text_aug = eda(df['content'][idx])\n",
    "    \n",
    "    df_merge = pd.DataFrame({\n",
    "        'content' : text_aug,\n",
    "        'individual' : [df['individual'][idx]] * len(text_aug),\n",
    "        'groups' : [df['groups'][idx]] * len(text_aug),\n",
    "        'religion/creed' : [df['religion/creed'][idx]] * len(text_aug),\n",
    "        'race/ethnicity' : [df['race/ethnicity'][idx]] * len(text_aug),\n",
    "        'politics' : [df['politics'][idx]] * len(text_aug)\n",
    "    })\n",
    "    df_augumentation = pd.concat([df_augumentation, df_merge], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augumentation.to_csv(\"religion_creed_aug.csv\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
